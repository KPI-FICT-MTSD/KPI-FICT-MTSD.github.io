==========
Unit-тести
==========

Давайте поговоримо сьогодні про тести. Тести — це круто, вони дозволяють нам
бути певними у тому, що наш код працює і працює коректно. Чому ж тоді люди не
пишуть тести? Які відмазки я чув/використовував?

- а, це проект на один раз, я його здам і забуду. Дуже часто це можна
  застосувати до лаб в універі, яких у нас багато, часу на них мало. А
  витрачати час ще й на якісь тести? Щоб що? Щоб викинути після здачі?
- написання тестів забирає достатньо багато часу. Як мінімум — стільки ж часу,
  скільки й безпосередньо розробка фічі. А тепер питання: до вас щодня
  приходить бізнес і питає: ну коли фіча. А ти їм — ну, я тести пишу. Чи
  сподобається це бізнесу? Не кожному.
- Написання тестів — це, насправді, нетривіальна задача. Просто написати тести
  — недостатньо. Важливо, щоб ці тести працювали і приносили користь. Написати
  тести правильно — не так вже й просто. А ми, люди, любимо уникати складнощів
  і йти по лінії найменшого спротиву.

Але все ж тести писати варто

По-перше, тести дають нам впевненість у завтрашньому дні. Що це означає? Хто
може розказати? (ніхто непомітно не поламає код)

По-друге, тести дають нам впевненість у тому коді, який ми пишемо. Хто як
думає, яким саме чином? Може хтось може навести приклади? (а наш код точно
працює як слід?)

Ну і третя очевидна причина, яка випливає з перших двох — якщо ми потратимо час
на тести сьогодні, то завтра нам буде простіше вносити зміни до нашого коду —
ми бачитимемо наглядно, як ці зміни впливають на систему загалом, чи вони її
ламають, чи ні, тощо.

Також тести пишуть для того, щоб задокументувати те, як ви збираєтеся
використовувати ваш код. Хороші тести не тільки перевіряють коректність роботи
коду, а й демонструють те, як автор очікував, що його код використовуватимуть.
В Python навіть для цього є окремий модуль doctest.
(https://docs.python.org/3/library/doctest.html), який дозволяє описувати
частину тестів як документацію вашого коду.

Як ми вже говорили вище, часто буває складно написати тести. Ми чесно хочемо
написати тест, але коли починаємо писати розуміємо, що виходить якась діч. Ми
не можемо протестувати один модуль без іншого, а інший — без третього і так
далі… В таких випадках є 2 виходи: перший — це забити болт і не писати тести. і
другий — це переосмислити архітектуру та зв’язки між компонентами так, щоб їх
можна було доволі легко протестувати. Таким чином написання тестів до вашого
коду дозволяє в тому числі покращувати не лише коректність, а й якість вашого
коду.

Це все звучить гарно, але що ж там на практиці? Чи дійсно ці тести допомагають
на практиці впевненіше змінювати код.

От приклад з практики. Виникла потреба відрефакторити одну з ключових і дуже
важливих підсистем доволі великого проекту. Очевидно, що ціна помилки тут дуже
висока — якщо щось піде не так, компанія в буквальному сенсі може попасти під
великі штрафи. При цьому, те, як цей весь код працює, більш-менш розуміють десь
троє-четверо інженерів зі ста, що є в компанії. Інші сюди просто не залазять, і
знають тільки, що воно якось працює і працює правильно.

І от рефакторинг цього всього діла почали з того, що почали покривати тестами
усе те, що ними покрито не було. На першому етапі написали майже 3000 рядків
тестів, що покривали основні сценарії використання даної підсистеми. Десь через
півроку після цього рефакторинг було завершено і ще декілька місяців
відбувалася плавна міграція. Критичних збоїв не було, все працювало як
годинник.

Як думаєте, чи можна було цього досягти без написання тестів? Думаю, що так,
можна. Але помилок було би більше. І часу це все зайняло б не майже рік, а роки
2-3. Таким чином оця от інвестиція часу та ресурсів в тести сильно дозволила
скоротити і спростити таку непросту задачу. Тести рулять, ага.

Що ж, багато хто зверне увагу: в темі лекції говориться про якісь UNIT тести, а
тут я розповідаю про тести загалом. То що таке unit-тести, та які вони бувають
ще?

Загалом, є дві основні класифікації тестів. Одна із них — за скоупом, інша за
розміром.

За розміром тести поділяють на, несподівано, маленькі, середні і великі.
Поділяють за к-тю ресурсів, які цим тестам необхідні. Якщо тестами треба одне
ядро, трошки оперативки, і все — це малі тести. Середні — декілька ядер, чуть
більше оперативки. Умовно, один сервер. Великі ж тести просять декілька
серверів, мережу між ними, тощо. Основна суть такої класифікації у тому, що це
все можна обмежити, а не йняти на віру. Ми можемо обмежити кількість доступних
процесу ресурсів.

Ортогональною їй є класифікація за скоупом. Тут, думаю, багато хто
здогадується, що тести поділяються на Unit-тести, Integration тести та E2E
тести. Хтось бажає коротко розповісти у чому суть такого поділу?

Unit-тести — це зазвичай маленькі тести, які покривають функціонал, обмежений
скоупом одного модуля (юніта). Модулем в даному випадку може бути: функція,
клас, набір класів, тощо. Головне, щоб це модуль був одним логічним цілим.

Integration тести — взаємодія між модулями

E2E тести так називаються, бо тестують усю систему наскрізь, так ніби її
використовують реальні люди/користувачі.

Зазвичай unit-тести у нас маленькі. Integration — малі або середні. Ну а E2E
тести — великі. Але це співвідношення воно радше умовне і не завжди
справджується.

Очевидно, що чисто Unit-тестами неможливо перевірити коректність роботи усієї
системи в усіх випадках. Те ж саме стосується і виключно E2E тестів чи
Integration-тестів. Тільки комбінуючи прям усі-усі ці тести можна досягти
максимально повного і коректного покриття вашого функціоналу тестами.

Книга Software Engineering @ Google радить наступний розподіл тестів на проекті (числа приблизні):

- ~ 80% тестів — це unit-тести. Чому? Тому що, по-перше, ці unit-тести зазвичай
  невеликі. По-друге, вони (в ідеалі) покривають усі крайні випадки для того чи
  іншого модуля. Таким чином ми можемо відносно швидко переконатися, що усі наші
  модулі функціонують коректно.
- ~ 15% тестів — інтеграційні. Взаємодія між модулями важлива, але оскільки
  більшість сценаріїв покрито безпосередньо в самих модулях unit-тестами,
  інтеграційних тестів треба значно менше.
- І вершину піраміди ~5% — складають E2E тести. Вони зазвичай довгі, тому ними
  покривають тільки основні сценарії. Ну і більшість виключних ситуацій вже
  покрита тестами unit та інтеграційним

Ок, а що ж може піти не так, якщо ми не притримуватимемося цих рекомендацій?

Найбільш розповсюдженими сценаріями, коли щось пішло не так є т.з. морозивко та
пісочний годинник.

Коли у вас мало unit-тестів, чуть більше integration, але в основному ви
покладаєтеся на E2E тести чи взагалі ручні — скоріш за все, це свідчення того,
що unit-тести писати складно. Як ми пам’ятаємо, в свою чергу це може вказувати
на недоліки в архітектурі вашої програми, і очевидно, що підтримувати такий
проект складно. По-перше, ви тратите багато часу на тестування (ручками то
довго). А по-друге, якщо у вас все між собою настільки хитро переплетено, то
внести мінімальні зміни стає доволі проблематично. Особливо ще й так, щоб
нічого іншого не поламати.

Пісочний годинник — це коли у вас багато unit-тестів та E2E тестів. Це,
очевидно, краще, ніж морозивко, але все ж, чимала частка функціоналу тестується
повільними і ламкими E2E тестами.

Поки що ми зосередимося на unit-тестах, так як цих тестів у вас на проекті має
бути найбільше. Як же їх писати? Що таке хороший unit-тест?

Є `дуже крута доповідь Тайтуса та Хайрума <https://youtu.be/u5senBJUkPc>`_ на
конфі про C++ про те, як правильно писати unit-тести. Дуже рекомендую витратити
годину свого часу та подивитися її.

Вони виділяють такі ознаки гарно написаних unit-тестів:

- коректність,
- читабельність,
- повнота,
- демонстративність
- стійкість.

З коректністю все більш-менш зрозуміло (якщо 2+2 = 4, то наш тест не повинен
стверджувати, що 2 + 2 = 5, просто тому, щоб бути “зелененьким”, або ж
проходити).

Читабельність. Ми пишемо тести, щоб не допускати помилок. Якщо наш тест падає —
вірогідно, десь в коді помилка. А тепер уявіть, якщо при цьому той тест, що
падає, займає 3 екрани макаронного коду, в якому сам чорт ногу зломить. Тому ми
повинні писати тести так, щоб можна було лише кинути оком і швидко зрозуміти,
що ж у цьому тесті відбувається.

Повнота. Ми повинні в наших тестах покривати увесь функціонал, а не лише оцю
одну функцію, бо її покрити легко, а не решту забити. Також ми в своїх тестах
не повинні покривати нічого зайвого. Ви пишете логіку довгої арифметики на
списках і використовуєте вбудовані в стандартну бібліотеку вашої мови
програмування вектори. Так от, на сам вектор вам писати тести не треба, тому що
це дурна трата часу та ресурсів. Їх вже, найімовірніше, протестували розробники
бібліотеки, а по-друге, якщо ви не довіряєте стандартній бібліотеці, то чому ви
довіряєте самій мові? Чому ви довіряєте комп’ютеру, на якому це виконується?

Демонстративність. Про це вже говорили. Тести повинні показувати, як ваш код
використовувати, а не юзати чорні ходи, виключно для тестів. Інакше ви не
тестуєте ту поведінку, яку обіцяєте у своєму модулі.

Стійкість. Наші тести повинні бути стійкими до змін. Тести повинні падати тоді
і тільки тоді, коли змінилася поведінка, яку цей тест покриває. Інакше, якщо
тест падає випадковим чином — то це не тест, а генератор випадкових чисел. Про
них ви можете дізнатися, здається, у Марковського, там теж все дуже “весело”.

Взагалі структура тесту доволі проста і складається із 3х частин:

1. Arrange
2. Act
3. Assert

Спершу ми готуємо дані для тесту (arrange). Потім, власне, виконуємо якусь дію
(act). І насамкінець — перевіряємо результат (assert).

Пам’ятаєте, ми говорили про стійкість тестів? Що ж це означає? Давайте
подумаємо стосовно того, коли нам треба змінювати код, а коли тести? Код ми
змінюємо при рефакторингу, при виправленні багів, додаванні нових фіч чи зміні
поведінки системи загалом. Але при цьому, якщо подумати, то змінювати тести нам
потрібно тільки в тому випадку, коли ми змінюємо поведінку системи.

Звідси випливає те, що тести ми пишемо так, щоб потім їх не чіпати і не
переписувати. Як ми можемо цього досягти?

По-перше, як уже згадувалося, тести повинні використовувати те саме АПІ, що й
користувачі вашого модуля. Таким чином, ми убезпечуємо себе від того, що при
рефакторингу у нас усе посипеться.

Ще одним підходом, який дозволить зробити наші тести стійкішими, є тестування
станів, а не взаємодій. Якщо ми тестуємо взаємодії (типу викликали такий-то
метод) у наших unit-тестах, ми можемо проґавити неприємні баги. Також взаємодії
у нашому модулі можуть бути тими самими деталями реалізації.

Ну і звісно, що в ідеалі тести мають бути настільки простими, наскільки це
можливо. А значить — логіки в них ми повинні уникати. По-перше, це робить наші
тести значно складнішими. По-друге, який сенс в тестах, якщо ми дублюємо логіку
модуля в нашому тесті? Ми ж не перевіряємо, що ми правильно скопіювали код?
По-третє, у цій логіці можуть бути помилки.

Інколи у нас буває доволі складна взаємодія між частинами нашого модуля, і щоб
не ускладнювати своє життя, щоб наші тести були простішими нам інколи
доводиться користуватися т.з. дублерами, або ж test-doubles. Приклад: наш
модуль взаємодіє з базою даних. Для кожного маленького тесту піднімати БД — це
прям явний перебір. Замість цього можна написати дубера, який матиме той самий
інтерфейс, що і база даних, але при цьому буде доволі простим. Взагалі,
розрізняють 3 види таких дублерів: Fakes, Stubs та Mocks.

Найпростішим для розуміння є дублер-Fake. Це альтернативна реалізація
компонента. Ця альтернативна реалізація повністю повторює інтерфейс реального
компонента (навіть помилки, які він викидає), тож наш код (і тести) не знають,
чи працюють вони з реальним компонентом чи з фейком. 

    Наприклад, робота з ФС на диску — повільна, порівняно з оперативною
    пам’яттю. Тож ми для наших тестів, щоб їх пришвидшити, реалізовуємо
    фейковий компонент, який повторює інтерфейс реальної ФС, але тримає усі
    дані в оперативці.

Очевидно, що є нюанс: ці фейки потрібно підтримувати.  Щоразу як змінюється
поведінка інтерфейсу реальної ФС, нам потрібно внести відповідні зміни і у наш
фейк. Ну і тестувати фейки для тестів теж потрібно — раптом виявиться що через
декілька місяців фейк працює по-одному, а реальний компонент — інакше.

Стаби. Мабуть, найулюбленіша штука для написання юніт тестів. Дозволяє швидко і
просто захардкодити поведінку для тестів. Звучить круто, але неконтрольоване їх
використання приведе до того, що у ваших тестах реальної поведінки не буде —
лише захардкожені стаби

Інколи неможливо тестувати стани. Наприклад, як ви будете перевіряти, що
відправили email? Це можна зробити в E2E тестах, піднявши тестовий SMTP сервер,
і перевіряти, що там відправлялося. Але в unit-тестах це перевірити нереально —
пам’ятаємо, ці тести мають бути малими та швидкими. Тому єдиним виходом є
тестування взаємодій. Для unit-тесту достатньо перевірити, що ми викликали
функцію відправки  email в нашій логіці, це ми можемо зробити за допомогою
моків.

Узагалі, якщо є можливість при тестуванні використовувати реальні компоненти —
використовуйте їх. Якщо ні — то в порядку пріоритетів використовуйте фейки,
стаби і моки.

Окей, ми вміємо писати тести і запускати їх. Ми робимо проект, пишемо тести,
насолоджуємося процесом. Завжди перед git push запускаємо тести, і
переконуємося, що все працює. Але в якийсь момент вас відволікли і ви забули
запустити тести. І всьо. Все поламане. Інші люди теж провтикали — і кількість
помилок збільшується. І от сидите ви і думаєте, як же так трапилося, і як цього
можна було уникнути?

В найпростішому виді CI (Continuous Integration) вирішує проблему того, що
хтось щось забув зробити. Тож поки можемо і будемо її розглядати як автоматичну
запускалку для тестів. Вцілому CI/CD — це більш складна процедура, з
конвеєрами, збіркою артефактів, тощо. Але на поточному етапі зосередимося на
тому, що нам принесе користь вже зараз.

Інструментів, за допомого. яких можна налаштувати процес CI — безліч. Jenkins,
GitlabCI, GitHub Actions. CircleCI, Travis, Bamboo, тощо. У них усіх є свої
переваги та недоліки. Ми в рамках цього курсу користуємося GitHub, тож будемо
використовувати вже інтегровані GitHub Actions.

Конфігурація наших GitHub actions складається з наступних елементів:

#. назви конвеєра (сценарію, який ми запускаємо)
#. вказання умов для запуску цього сценарію (коли запускаємо)
#. опису середовищ, в яких ми запускаємо наші перевірки
#. опису кроків, які ми запускаємо

.. code-block:: yaml

   name: Run linters and tests  # (1)
   on:  # (2)
     push:
       branches:
         - master
     pull_request:
       branches:
         - master
   jobs:
     lint:
       runs-on: ubuntu-latest  # (3)
       strategy:               # (3)
         matrix:
           node-version:
             - 8.x
             - 14.x
       needs: [prepare]
       steps:  # (4)
         - uses: actions/checkout@v2
         - name: Use Node.js ${{ matrix.node-version }}
           uses: actions/setup-node@v1
           with:
             node-version: ${{ matrix.node-version }}
         - run: npm ci
           working-directory: ./app
         - run: npm run lint --if-present
           working-directory: ./app

Взагалі, як це все робити?

#. Починаєте проект.
#. Відкриваєте документацію по `GitHub Actions <https://docs.github.com/en/actions>`_
#. знаходите сценарій, який вам найбільше підходить
#. Ctrl+C, Ctrl+V
#. Адаптуєте під свої реалії
